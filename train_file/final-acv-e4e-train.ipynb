{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2177371,"sourceType":"datasetVersion","datasetId":1307206},{"sourceId":5815323,"sourceType":"datasetVersion","datasetId":3341018}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchvision\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git\n!pip install tensorflow-io\n!pip install Ninja\n!pip install dlib\n!pip install cog","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n!git clone https://github.com/wty-ustc/HairCLIP.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/HairCLIP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/omertov/encoder4editing.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"!gdown --id '1gof8kYc_gDLUT4wQlmUdAtPnQIlCO26q' -O \"/kaggle/working/HairCLIP/mapper/datasets/train_faces.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id '1j7RIfmrCoisxx3t-r-KC02Qc8barBecr' -O '/kaggle/working/HairCLIP/mapper/datasets/test_faces.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntrain_latents = torch.load(\"/kaggle/working/HairCLIP/mapper/datasets/train_faces.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_latents = torch.load(\"/kaggle/working/HairCLIP/mapper/datasets/test_faces.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_latents.shape, test_latents.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checkpoints","metadata":{}},{"cell_type":"code","source":"!gdown --id '1cUv_reLE6k3604or78EranS7XzuVMWeO' -O \"/kaggle/working/HairCLIP/pretrained_models/e4e_ffhq_encode.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id '1pts5tkfAcWrg4TpLDu6ILF5wHID32Nzm' -O \"/kaggle/working/HairCLIP/pretrained_models/stylegan2-ffhq-config-f.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id '1hqZT6ZMldhX3M_x378Sm4Z2HMYr-UwQ4' -O \"/kaggle/working/HairCLIP/pretrained_models/hairclip.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id '1FS2V756j-4kWduGxfir55cMni5mZvBTv' -O \"/kaggle/working/HairCLIP/pretrained_models/model_ir_se50.pth\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -P /kaggle/working/HairCLIP/pretrained_models http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 # DOWNLOAD LINK\n\n!bunzip2 /kaggle/working/HairCLIP/pretrained_models/shape_predictor_68_face_landmarks.dat.bz2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LatentsDatasetInference.py","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport sys\nimport numpy as np\nimport clip\nimport torch\nimport random\nfrom PIL import Image\nimport torchvision.transforms as transforms\nsys.path.insert(0, \"/kaggle/working/HairCLIP/mapper\")\nfrom training import train_utils\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LatentsDatasetInference(Dataset):\n    def __init__(self, latents, opts):\n        self.latents = latents\n        self.opts = opts\n\n        if self.opts.editing_type in ['hairstyle', 'both'] and self.opts.input_type.split('_')[0] == 'text':\n            with open(self.opts.hairstyle_description, \"r\") as fd:\n                self.hairstyle_description_list = fd.read().splitlines()\n            self.hairstyle_list = [single_hairstyle_description[:-9] for single_hairstyle_description in self.hairstyle_description_list]\n        if self.opts.editing_type in ['color', 'both'] and self.opts.input_type.split('_')[-1] == 'text':\n            self.color_list = [single_color_description.strip()+' ' for single_color_description in self.opts.color_description.split(',')]\n        if self.opts.editing_type in ['hairstyle', 'both'] and self.opts.input_type.split('_')[0] == 'image':\n            self.out_domain_hairstyle_img_path_list = sorted(train_utils.make_dataset(self.opts.hairstyle_ref_img_test_path))\n        if self.opts.editing_type in ['color', 'both'] and self.opts.input_type.split('_')[-1] == 'image':\n            self.out_domain_color_img_path_list = sorted(train_utils.make_dataset(self.opts.color_ref_img_test_path))\n\n        self.image_transform = transforms.Compose([transforms.Resize((1024, 1024)), transforms.ToTensor(),transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\n\n    def manipulate_hairstyle(self, index):\n        if self.opts.input_type.split('_')[0] == 'text':\n            color_text_embedding_list = [torch.Tensor([0]) for i in range(len(self.hairstyle_list))]\n            color_tensor_list = [torch.Tensor([0]) for i in range(len(self.hairstyle_list))]\n            hairstyle_tensor_list = [torch.Tensor([0]) for i in range(len(self.hairstyle_list))]\n            selected_hairstyle_description_list = [single_hairstyle_description+'hairstyle' for single_hairstyle_description in self.hairstyle_list]\n            hairstyle_text_embedding_list = [torch.cat([clip.tokenize(selected_hairstyle_description)])[0] for selected_hairstyle_description in selected_hairstyle_description_list]\n        elif self.opts.input_type.split('_')[0] == 'image':\n            color_text_embedding_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            color_tensor_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            hairstyle_text_embedding_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            selected_hairstyle_description_list = ['hairstyle_out_domain_ref' for i in range(self.opts.num_of_ref_img)]\n            hairstyle_tensor_list = [self.image_transform(Image.open(random.choice(self.out_domain_hairstyle_img_path_list))) for i in range(self.opts.num_of_ref_img)]\n        return self.latents[index], hairstyle_text_embedding_list, color_text_embedding_list, selected_hairstyle_description_list, hairstyle_tensor_list, color_tensor_list\n\n\n    def manipulater_color(self, index):\n        if self.opts.input_type.split('_')[-1] == 'text':\n            hairstyle_text_embedding_list = [torch.Tensor([0]) for i in range(len(self.color_list))]\n            hairstyle_tensor_list = [torch.Tensor([0]) for i in range(len(self.color_list))]\n            color_tensor_list = [torch.Tensor([0]) for i in range(len(self.color_list))]\n            selected_color_description_list = [single_color_description+'hair' for single_color_description in self.color_list]\n            color_text_embedding_list = [torch.cat([clip.tokenize(selected_color_description)])[0] for selected_color_description in selected_color_description_list]\n        elif self.opts.input_type.split('_')[-1] == 'image':\n            hairstyle_text_embedding_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            hairstyle_tensor_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            color_text_embedding_list = [torch.Tensor([0]) for i in range(self.opts.num_of_ref_img)]\n            selected_color_description_list = ['color_out_domain_ref' for i in range(self.opts.num_of_ref_img)]\n            color_tensor_list = [self.image_transform(Image.open(random.choice(self.out_domain_color_img_path_list))) for i in range(self.opts.num_of_ref_img)]\n        return self.latents[index], hairstyle_text_embedding_list, color_text_embedding_list, selected_color_description_list, hairstyle_tensor_list, color_tensor_list\t\t\n\n\n    def manipulater_hairstyle_and_color(self, index):\n        returned_latent, hairstyle_text_embedding_list, _, selected_hairstyle_description_list, hairstyle_tensor_list, _ = self.manipulate_hairstyle(index)\n        _, _, color_text_embedding_list, selected_color_description_list, _, color_tensor_list = self.manipulater_color(index)\n        hairstyle_text_embedding_final_list = [hairstyle_text_embedding for hairstyle_text_embedding in hairstyle_text_embedding_list for i in color_text_embedding_list]\n        color_text_embedding_final_list = [color_text_embedding for i in hairstyle_text_embedding_list for color_text_embedding in color_text_embedding_list]\n        selected_description_list = [f'{selected_hairstyle_description}-{selected_color_description}' for selected_hairstyle_description in selected_hairstyle_description_list for selected_color_description in selected_color_description_list]\n        hairstyle_tensor_final_list = [hairstyle_tensor for hairstyle_tensor in hairstyle_tensor_list for i in color_tensor_list]\n        color_tensor_final_list = [color_tensor for i in hairstyle_tensor_list for color_tensor in color_tensor_list]\n        return returned_latent, hairstyle_text_embedding_final_list, color_text_embedding_final_list, selected_description_list, hairstyle_tensor_final_list, color_tensor_final_list\n\n\n    def __len__(self):\n        return self.latents.shape[0]\n\n    def __getitem__(self, index):\n        if self.opts.editing_type == 'hairstyle':\n            return self.manipulate_hairstyle(index)\n        elif self.opts.editing_type == 'color':\n            return self.manipulater_color(index)\n        elif self.opts.editing_type == 'both':\n            return self.manipulater_hairstyle_and_color(index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict.py","metadata":{}},{"cell_type":"code","source":"import sys\nimport tempfile\nfrom argparse import Namespace\n\nimport dlib\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom cog import BasePredictor, Path, Input\nfrom criteria.parse_related_loss import average_lab_color_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.insert(0, \"encoder4editing\")\nfrom models.psp import pSp\nfrom utils.alignment import align_face","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.insert(0, \"mapper\")\n# from mapper.datasets.latents_dataset_inference import LatentsDatasetInference\nfrom mapper.hairclip_mapper import HairCLIPMapper\nfrom mapper.options.test_options import TestOptions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nwith open(\"mapper/hairstyle_list.txt\") as infile:\n    HAIRSTYLE_LIST = sorted([line.rstrip() for line in infile])\n\n\nclass Predictor_e4e(BasePredictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        # use e4e to get latent code for an input image\n        e4e_model_path = \"pretrained_models/e4e_ffhq_encode.pt\"\n        e4e_ckpt = torch.load(e4e_model_path, map_location=\"cpu\")\n        e4e_opts = e4e_ckpt[\"opts\"]\n        e4e_opts[\"checkpoint_path\"] = e4e_model_path\n        e4e_opts = Namespace(**e4e_opts)\n\n        self.e4e_net = pSp(e4e_opts)\n        self.e4e_net.eval()\n        self.e4e_net.cuda()\n        print(\"e4e model successfully loaded!\")\n        \n        self.img_transforms = transforms.Compose(\n            [\n                transforms.Resize((256, 256)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n            ]\n        )\n\n        # HairClip model\n        checkpoint_path = \"pretrained_models/hairclip.pt\"\n        self.ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n        \n    \n    def predict(\n        self,\n        image: Path = Input(\n            description=\"Input image. Image will be aligned and resized. Output will be the \"\n            \"concatenation of the inverted input and the image with edited hair.\"\n        ),\n        editing_type: str = Input(\n            choices=[\"hairstyle\", \"color\", \"both\"],\n            default=\"hairstyle\",\n            description=\"Edit hairstyle or color or both.\",\n        ),\n        input_type: str = Input(\n            choices = [\"text\", \"image\", \"text_image\", \"image_text\"],\n            description=\"(1)_(2):  (1) for hairstyle and (2) for color\"\n        ),\n        hairstyle_description: str = Input(\n            choices=HAIRSTYLE_LIST,\n            default=None,\n            description=\"Hairstyle text prompt. \"\n            \"Valid if input_type is text or text_image.\",\n        ),\n        hairstyle_ref_img_test_path: Path = Input(),\n        \n        color_description: str = Input(\n            default=None,\n            description=\"Color text prompt, eg: purple, red, orange. \"\n            \"Valid if editing_type is color or both.\",\n        ),\n        color_ref_img_test_path: Path = Input()\n    ) -> Path:\n        \n        editing_type_ = str(editing_type).split(\".\")[-1]\n        hairstyle_description_ = str(hairstyle_description).split(\".\")[-1]\n        \n\n        if editing_type_ == \"both\":\n            assert (\n                hairstyle_description_ is not None and color_description is not None\n            ), (\"Please provide description \" \"for both hairstyle and color.\")\n        elif editing_type_ == \"hairstyle\":\n            assert (\n                hairstyle_description_ is not None\n            ), \"Please provide description for hairstyle.\"\n        else:\n            assert (\n                color_description is not None\n            ), \"Please provide description for color.\"\n\n        opts = self.ckpt[\"opts\"]\n        opts = Namespace(**opts)\n        \n        opts.num_of_ref_img = 5\n        opts.editing_type = editing_type_\n        opts.input_type = input_type\n        opts.color_description = color_description\n        if hairstyle_description is not None:\n            with open(\"hairstyle_description.txt\", \"w\") as file:\n                file.write(hairstyle_description_)\n\n            opts.hairstyle_description = \"hairstyle_description.txt\"\n        opts.color_ref_img_test_path = color_ref_img_test_path\n        opts.hairstyle_ref_img_test_path = hairstyle_ref_img_test_path\n        \n        opts.checkpoint_path = \"pretrained_models/hairclip.pt\"\n        opts.parsenet_weights = \"pretrained_models/parsenet.pth\"\n        opts.stylegan_weights = \"pretrained_models/stylegan2-ffhq-config-f.pt\"\n        opts.ir_se50_weights = \"pretrained_models/model_ir_se50.pth\"\n        net = HairCLIPMapper(opts)\n        net.eval()\n        net.cuda()\n\n        # first align, resize image and get latent code\n        input_image = run_alignment(str(image))\n        resize_dims = (256, 256)\n        input_image.resize(resize_dims)\n        transformed_image = self.img_transforms(input_image)\n\n        with torch.no_grad():\n\n            images, latents = run_on_batch_e4e(\n                transformed_image.unsqueeze(0), self.e4e_net\n            )\n            print(\"Latent code calculated!\")\n            print(f\"Latent code shape: {latents.shape}\")\n\n        dataset = LatentsDatasetInference(latents=latents.cpu(), opts=opts)\n        dataloader = DataLoader(dataset)\n\n        average_color_loss = (\n            average_lab_color_loss.AvgLabLoss(opts).to(self.device).eval()\n        )\n\n        out_path = Path(\"/kaggle/working/output.png\")\n\n        for input_batch in tqdm(dataloader):\n\n            with torch.no_grad():\n\n                (\n                    w,\n                    hairstyle_text_inputs_list,\n                    color_text_inputs_list,\n                    selected_description_tuple_list,\n                    hairstyle_tensor_list,\n                    color_tensor_list,\n                ) = input_batch\n                hairstyle_text_inputs = hairstyle_text_inputs_list[0]\n                color_text_inputs = color_text_inputs_list[0]\n                selected_description = selected_description_tuple_list[0][0]\n                hairstyle_tensor = hairstyle_tensor_list[0]\n                color_tensor = color_tensor_list[0]\n                \n                \n                w = w.cuda().float()\n                hairstyle_text_inputs = hairstyle_text_inputs.cuda()\n                color_text_inputs = color_text_inputs.cuda()\n                hairstyle_tensor = hairstyle_tensor.cuda()\n                color_tensor = color_tensor.cuda()\n                \n                if hairstyle_tensor.shape[1] != 1:\n                    hairstyle_tensor_hairmasked = (\n                        hairstyle_tensor * average_color_loss.gen_hair_mask(hairstyle_tensor)\n                    )\n                else:\n                    hairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                    \n                if color_tensor.shape[1] != 1:\n                    color_tensor_hairmasked = (\n                        color_tensor * average_color_loss.gen_hair_mask(color_tensor)\n                    )\n                else:\n                    color_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                    \n                result_batch = run_on_batch(\n                    w,\n                    hairstyle_text_inputs,\n                    color_text_inputs,\n                    hairstyle_tensor_hairmasked,\n                    color_tensor_hairmasked,\n                    net,\n                )\n\n                if (hairstyle_tensor.shape[1] != 1) and (color_tensor.shape[1] != 1):\n                    img_tensor = torch.cat([hairstyle_tensor, color_tensor], dim=3)\n                elif hairstyle_tensor.shape[1] != 1:\n                    img_tensor = hairstyle_tensor\n                elif color_tensor.shape[1] != 1:\n                    img_tensor = color_tensor\n                else:\n                    img_tensor = None\n\n                if img_tensor is not None:\n                    if img_tensor.shape[3] == 1024:\n                        couple_output = torch.cat(\n                            [\n                                result_batch[2][0].unsqueeze(0),\n                                img_tensor,\n                                result_batch[0][0].unsqueeze(0),\n                            ]\n                        )\n                    elif img_tensor.shape[3] == 2048:\n                        couple_output = torch.cat(\n                            [\n                                result_batch[2][0].unsqueeze(0),\n                                result_batch[0][0].unsqueeze(0),\n                                img_tensor[:, :, :, 0:1024],\n                                img_tensor[:, :, :, 1024::],\n                            ]\n                        )\n                        couple_output = torch.cat(\n                            [\n                                result_batch[2][0].unsqueeze(0),\n                                result_batch[0][0].unsqueeze(0),\n                                img_tensor[:, :, :, 0:1024],\n                                img_tensor[:, :, :, 1024::],\n                            ]\n                        )\n                else:\n                    couple_output = torch.cat(\n                        [\n                            result_batch[2][0].unsqueeze(0),\n                            result_batch[0][0].unsqueeze(0),\n                        ]\n                    )\n                    \n                torchvision.utils.save_image(\n                    couple_output, str(out_path), normalize=True\n                )\n\n                \n        return out_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datFile = \"/kaggle/working/HairCLIP/pretrained_models/shape_predictor_68_face_landmarks.dat\"\ndef run_alignment(image_path):\n    predictor = dlib.shape_predictor(datFile)\n    aligned_image = align_face(filepath=image_path, predictor=predictor)\n    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n    return aligned_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_on_batch_e4e(inputs, net):\n    images, latents = net(\n        inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True\n    )\n    return images, latents","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_on_batch(\n    inputs,\n    hairstyle_text_inputs,\n    color_text_inputs,\n    hairstyle_tensor_hairmasked,\n    color_tensor_hairmasked,\n    net,\n):\n    w = inputs\n    with torch.no_grad():\n        w_hat = w + 0.1 * net.mapper(\n            w,\n            hairstyle_text_inputs,\n            color_text_inputs,\n            hairstyle_tensor_hairmasked,\n            color_tensor_hairmasked,\n        )\n        x_hat, w_hat = net.decoder(\n            [w_hat],\n            input_is_latent=True,\n            return_latents=True,\n            randomize_noise=False,\n            truncation=1,\n        )\n        x, _ = net.decoder(\n            [w], input_is_latent=True, randomize_noise=False, truncation=1\n        )\n        result_batch = (x_hat, w_hat, x)\n    return result_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check image","metadata":{}},{"cell_type":"markdown","source":"## Input_image","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define a function to remove a directory and its contents recursively\ndef remove_folder(folder_path):\n    shutil.rmtree(folder_path)\n\n# Example usage:\nif os.path.exists('/kaggle/working/Image'):\n    remove_folder('/kaggle/working/Image')\n    \n!git clone https://github.com/HongQuan2003/21522490_final_ACV.git /kaggle/working/Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as TF\nimport torchvision.io as io\n\ninput_image_path = \"/kaggle/working/Image/Input_image/IMG_0127.jpeg\"\ninput_image = io.read_image(input_image_path)\n\n# Plot the image\nplt.imshow(input_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rotate (if needed)**","metadata":{}},{"cell_type":"code","source":"rotated_input_image_path = \"/kaggle/working/rotated_input_image.jpeg\"\n\n# Rotate the image tensor by 90 degrees counter-clockwise\nrotated_input_image = torch.rot90(input_image, k=-1, dims=(1, 2))\n\n# Convert the rotated tensor back to a PIL Image\nrotated_input_image = TF.to_pil_image(rotated_input_image)\n\n# Save the rotated image\nrotated_input_image.save(rotated_input_image_path)\n\n# Assuming 'image_tensor' is your image tensor\ninput_image = io.read_image(rotated_input_image_path)\n\n# Plot the image\nplt.imshow(input_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()\n\ninput_image_path = rotated_input_image_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ref_image","metadata":{}},{"cell_type":"markdown","source":"### hairstyle","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as TF\nimport torchvision.io as io\nimport os\n\nref_image_path = \"/kaggle/input/flickr-faces-hq-dataset-ffhq/images1024x1024/00000.png\"\n\nif os.path.exists('/kaggle/working/reference_image_hairstyle') == False:\n    os.mkdir('/kaggle/working/reference_image_hairstyle')\n    \nref_image = io.read_image(ref_image_path)\nref_image = TF.to_pil_image(ref_image)\nref_image.save(\"/kaggle/working/reference_image_hairstyle/ref_img.jpeg\")\n\n\nref_image = io.read_image(\"/kaggle/working/reference_image_hairstyle/ref_img.jpeg\")\n# Plot the image\nplt.imshow(ref_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rotate (if needed)**","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:03:18.782844Z","iopub.execute_input":"2024-04-22T05:03:18.783553Z","iopub.status.idle":"2024-04-22T05:03:18.789535Z","shell.execute_reply.started":"2024-04-22T05:03:18.783518Z","shell.execute_reply":"2024-04-22T05:03:18.788306Z"}}},{"cell_type":"code","source":"# Rotate the image tensor by 90 degrees counter-clockwise\nrotated_ref_image = torch.rot90(ref_image, k=-1, dims=(1, 2))\n\n# Convert the rotated tensor back to a PIL Image\nrotated_ref_image = TF.to_pil_image(rotated_ref_image)\n\n# Save the rotated image\nrotated_ref_image.save(\"/kaggle/working/reference_image_hairstyle/ref_img.jpeg\")\n\n# Assuming 'image_tensor' is your image tensor\nref_image = io.read_image(\"/kaggle/working/reference_image_hairstyle/ref_img.jpeg\")\n\n# Plot the image\nplt.imshow(ref_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Color","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as TF\nimport torchvision.io as io\nimport os\n\nref_image_path = \"/kaggle/input/celebahq-resized-256x256/celeba_hq_256/00003.jpg\"\n\nif os.path.exists('/kaggle/working/reference_image_color') == False:\n    os.mkdir('/kaggle/working/reference_image_color')\n    \nref_image = io.read_image(ref_image_path)\nref_image = TF.to_pil_image(ref_image)\nref_image.save(\"/kaggle/working/reference_image_color/ref_img.jpeg\")\n\n\nref_image = io.read_image(\"/kaggle/working/reference_image_color/ref_img.jpeg\")\n# Plot the image\nplt.imshow(ref_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rotate (if needed)**\n","metadata":{}},{"cell_type":"code","source":"# Rotate the image tensor by 90 degrees counter-clockwise\nrotated_ref_image = torch.rot90(ref_image, k=-1, dims=(1, 2))\n\n# Convert the rotated tensor back to a PIL Image\nrotated_ref_image = TF.to_pil_image(rotated_ref_image)\n\n# Save the rotated image\nrotated_ref_image.save(\"/kaggle/working/reference_image_color/ref_img.jpeg\")\n\n# Assuming 'image_tensor' is your image tensor\nref_image = io.read_image(\"/kaggle/working/reference_image_color/ref_img.jpeg\")\n\n# Plot the image\nplt.imshow(ref_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Edit hair with HairClip_e4e","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/HairCLIP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Hair_clip_e4e = Predictor_e4e()\nHair_clip_e4e.setup()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\noutpath = Hair_clip_e4e.predict(\n        image = input_image_path,\n        editing_type = \"both\",\n        input_type = \"image_text\",\n        hairstyle_description = \"dreadlocks hairstyle\",\n        color_description = \"purple\",\n        hairstyle_ref_img_test_path = \"/kaggle/working/reference_image_hairstyle\",\n        color_ref_img_test_path = \"/kaggle/working/reference_image_color\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as TF\nimport torchvision.io as io\n\n# Assuming 'image_tensor' is your image tensor\noutput_image = io.read_image(str(outpath))\n\n# Plot the image\nplt.imshow(output_image.permute(1, 2, 0))\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LatentDataset","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/HairCLIP","metadata":{"execution":{"iopub.status.busy":"2024-05-02T10:35:03.830756Z","iopub.execute_input":"2024-05-02T10:35:03.831284Z","iopub.status.idle":"2024-05-02T10:35:03.838513Z","shell.execute_reply.started":"2024-05-02T10:35:03.831243Z","shell.execute_reply":"2024-05-02T10:35:03.837539Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/HairCLIP\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport numpy as np\nimport clip\nimport torch\nimport random\nimport sys\nfrom PIL import Image\nimport torchvision.transforms as transforms\nsys.path.insert(0, \"/kaggle/working/HairCLIP/mapper\")\nfrom mapper.training import train_utils\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-05-02T10:35:30.663384Z","iopub.execute_input":"2024-05-02T10:35:30.663792Z","iopub.status.idle":"2024-05-02T10:35:30.671115Z","shell.execute_reply.started":"2024-05-02T10:35:30.663759Z","shell.execute_reply":"2024-05-02T10:35:30.670173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class LatentsDataset(Dataset):\n\n    def __init__(self, latents, opts, status='train'):\n        self.latents = latents\n        self.opts = opts\n        self.status = status\n        assert (self.opts.hairstyle_manipulation_prob+self.opts.color_manipulation_prob+self.opts.both_manipulation_prob) <= 1\n        with open(self.opts.hairstyle_description, \"r\") as fd:\n            self.hairstyle_description_list = fd.read().splitlines()\n\n        self.hairstyle_list = [single_hairstyle_description[:-9] for single_hairstyle_description in self.hairstyle_description_list]\n        self.color_list = [single_color_description.strip()+' ' for single_color_description in self.opts.color_description.split(',')]\n        self.image_transform = transforms.Compose([transforms.Resize((1024, 1024)), transforms.ToTensor(),transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n        if self.status == 'train':\n            self.out_domain_hairstyle_img_path_list = sorted(train_utils.make_dataset(self.opts.hairstyle_ref_img_train_path))\n            self.out_domain_color_img_path_list = sorted(train_utils.make_dataset(self.opts.color_ref_img_train_path))\n        else:\n            self.out_domain_hairstyle_img_path_list = sorted(train_utils.make_dataset(self.opts.hairstyle_ref_img_test_path))\n            self.out_domain_color_img_path_list = sorted(train_utils.make_dataset(self.opts.color_ref_img_test_path))\n\n\n    def manipulate_hairstyle(self, index):\n        color_text_embedding = torch.Tensor([0])\n        color_tensor = torch.Tensor([0])\n        if random.random() < self.opts.hairstyle_text_manipulation_prob:\n            selected_hairstyle_description = np.random.choice(self.hairstyle_list)+'hairstyle'\n            selected_description = selected_hairstyle_description\n            hairstyle_text_embedding = torch.cat([clip.tokenize(selected_hairstyle_description)])[0]\n            hairstyle_tensor = torch.Tensor([0])\n        else:\n            hairstyle_text_embedding = torch.Tensor([0])\n            img_pil = Image.open(random.choice(self.out_domain_hairstyle_img_path_list))\n            hairstyle_tensor = self.image_transform(img_pil)\n            selected_description = 'hairstyle_out_domain_ref'\n        return self.latents[index], hairstyle_text_embedding, color_text_embedding, selected_description, hairstyle_tensor, color_tensor\n\n    def manipulater_color(self, index):\n        hairstyle_text_embedding = torch.Tensor([0])\n        hairstyle_tensor = torch.Tensor([0])\n        selected_color_description = np.random.choice(self.color_list)+'hair'\n        if random.random() < self.opts.color_text_manipulation_prob:\n            selected_description = selected_color_description\n            color_text_embedding = torch.cat([clip.tokenize(selected_color_description)])[0]\n            color_tensor = torch.Tensor([0])\n        else:\n            color_text_embedding = torch.Tensor([0])\n            if random.random() < (self.opts.color_in_domain_ref_manipulation_prob/(1-self.opts.color_text_manipulation_prob)):\n                selected_description = 'color_in_domain_ref'\n                img_pil = Image.open(self.opts.color_ref_img_in_domain_path+selected_color_description+'/'+str(random.randint(0, (self.opts.num_for_each_augmented_color-1))).zfill(5)+'.jpg')\n                color_tensor = self.image_transform(img_pil)\n            else:\n                selected_description = 'color_out_domain_ref'\n                img_pil = Image.open(random.choice(self.out_domain_color_img_path_list))\n                color_tensor = self.image_transform(img_pil)\n        return self.latents[index], hairstyle_text_embedding, color_text_embedding, selected_description, hairstyle_tensor, color_tensor\n\n    def manipulater_hairstyle_and_color(self, index):\n        returned_latent, hairstyle_text_embedding, _, selected_hairstyle_description, hairstyle_tensor, _ = self.manipulate_hairstyle(index)\n        _, _, color_text_embedding, selected_color_description, _, color_tensor = self.manipulater_color(index)\n        selected_description = f'{selected_hairstyle_description}-{selected_color_description}'\n        return returned_latent, hairstyle_text_embedding, color_text_embedding, selected_description, hairstyle_tensor, color_tensor\n\n    def no_editing(self, index):\n        return self.latents[index], torch.Tensor([0]), torch.Tensor([0]), 'no_editing', torch.Tensor([0]), torch.Tensor([0])\n\n    def __len__(self):\n        return self.latents.shape[0]\n\n    def __getitem__(self, index):\n        function_list = ['self.manipulate_hairstyle(index)', 'self.manipulater_color(index)', 'self.manipulater_hairstyle_and_color(index)', 'self.no_editing(index)']\n        prob_array = np.array([self.opts.hairstyle_manipulation_prob, self.opts.color_manipulation_prob, self.opts.both_manipulation_prob, (1-self.opts.hairstyle_manipulation_prob-self.opts.color_manipulation_prob-self.opts.both_manipulation_prob)])\n        return eval(np.random.choice(function_list, replace=False, p=prob_array.ravel()))","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:16.029429Z","iopub.execute_input":"2024-05-02T11:04:16.030137Z","iopub.status.idle":"2024-05-02T11:04:16.053982Z","shell.execute_reply.started":"2024-05-02T11:04:16.030100Z","shell.execute_reply":"2024-05-02T11:04:16.053020Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# image_embedding_loss.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport clip\nimport torchvision.transforms as transforms\n\nclass ImageEmbddingLoss(torch.nn.Module):\n\n    def __init__(self):\n        super(ImageEmbddingLoss, self).__init__()\n        self.model, _ = clip.load(\"ViT-B/32\", device=\"cuda\")\n        self.transform = transforms.Compose([transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n        self.face_pool = torch.nn.AdaptiveAvgPool2d((224, 224))\n        self.cosloss = torch.nn.CosineEmbeddingLoss()\n\n    def forward(self, masked_generated, masked_img_tensor):\n#         print(f'masked_generated: {masked_generated.shape}, masked_img_tensor: {masked_img_tensor.shape}')\n        \n        masked_generated = self.face_pool(masked_generated)\n#         print(f'masked_generated: {masked_generated.shape}')\n              \n        masked_generated_renormed = self.transform(masked_generated * 0.5 + 0.5)\n#         print(f'masked_generated_renormed: {masked_generated_renormed.shape}')\n\n        masked_generated_feature = self.model.encode_image(masked_generated_renormed)\n#         print(f'masked_generated_feature: {masked_generated_feature.shape}')\n\n        masked_img_tensor = self.face_pool(masked_img_tensor)\n#         print(f'masked_img_tensor: {masked_img_tensor.shape}')\n\n        masked_img_tensor_renormed = self.transform(masked_img_tensor * 0.5 + 0.5)\n#         print(f'masked_img_tensor_renormed: {masked_img_tensor_renormed.shape}')\n        \n        masked_img_tensor_feature = self.model.encode_image(masked_img_tensor_renormed)\n#         print(f'masked_img_tensor_feature: {masked_img_tensor_feature.shape}')\n              \n        cos_target = torch.ones((masked_img_tensor.shape[0])).float().cuda()\n        similarity = self.cosloss(masked_generated_feature, masked_img_tensor_feature, cos_target).unsqueeze(0).unsqueeze(0)\n        return similarity\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# coach.py","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/HairCLIP","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:16.456170Z","iopub.execute_input":"2024-05-02T11:04:16.456796Z","iopub.status.idle":"2024-05-02T11:04:16.462270Z","shell.execute_reply.started":"2024-05-02T11:04:16.456755Z","shell.execute_reply":"2024-05-02T11:04:16.461398Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"/kaggle/working/HairCLIP\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport clip\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom criteria.parse_related_loss import bg_loss, average_lab_color_loss\nimport criteria.clip_loss as clip_loss\n# import criteria.image_embedding_loss as image_embedding_loss\nfrom criteria import id_loss\n\n# sys.path.insert(0, \"mapper\")\n# from mapper.datasets.latents_dataset import LatentsDataset\nfrom mapper.hairclip_mapper import HairCLIPMapper\nfrom mapper.training.ranger import Ranger\nfrom mapper.training import train_utils","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:16.593205Z","iopub.execute_input":"2024-05-02T11:04:16.593813Z","iopub.status.idle":"2024-05-02T11:04:16.599501Z","shell.execute_reply.started":"2024-05-02T11:04:16.593786Z","shell.execute_reply":"2024-05-02T11:04:16.598628Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class Coach:\n    def __init__(self, opts):\n        self.opts = opts\n        self.global_step = 0\n        self.device = 'cuda:0'\n        self.opts.device = self.device\n\n        # Initialize network\n        self.net = HairCLIPMapper(self.opts).to(self.device)\n\n        # Initialize loss\n        self.id_loss = id_loss.IDLoss(self.opts).to(self.device).eval()\n        self.clip_loss = clip_loss.CLIPLoss(opts)\n        self.latent_l2_loss = nn.MSELoss().to(self.device).eval()\n        self.background_loss = bg_loss.BackgroundLoss(self.opts).to(self.device).eval()\n        self.image_embedding_loss = ImageEmbddingLoss()\n        self.average_color_loss = average_lab_color_loss.AvgLabLoss(self.opts).to(self.device).eval()\n        self.maintain_color_for_hairstyle_loss = average_lab_color_loss.AvgLabLoss(self.opts).to(self.device).eval()\n\n        # Initialize optimizer\n        self.optimizer = self.configure_optimizers()\n\n        # Initialize dataset\n        self.train_dataset, self.test_dataset = self.configure_datasets()\n        self.train_dataloader = DataLoader(self.train_dataset,\n                                           batch_size=self.opts.batch_size,\n                                           shuffle=True,\n                                           num_workers=int(self.opts.workers),\n                                           drop_last=True)\n        self.test_dataloader = DataLoader(self.test_dataset,\n                                          batch_size=self.opts.test_batch_size,\n                                          shuffle=False,\n                                          num_workers=int(self.opts.test_workers),\n                                          drop_last=True)\n\n\n\n        # Initialize logger\n        log_dir = os.path.join(opts.exp_dir, 'logs')\n        os.makedirs(log_dir, exist_ok=True)\n        self.log_dir = log_dir\n        self.logger = SummaryWriter(log_dir=log_dir)\n\n        # Initialize checkpoint dir\n        self.checkpoint_dir = os.path.join(opts.exp_dir, 'checkpoints')\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        self.best_val_loss = None\n        if self.opts.save_interval is None:\n            self.opts.save_interval = self.opts.max_steps\n\n    def train(self):\n        self.net.train()\n        while self.global_step < self.opts.max_steps:\n            for batch_idx, batch in enumerate(self.train_dataloader):\n                self.optimizer.zero_grad()\n                w, hairstyle_text_inputs, color_text_inputs, selected_description_tuple, hairstyle_tensor, color_tensor = batch\n                selected_description = ''\n                for item in selected_description_tuple:\n                    selected_description+=item\n\n                w = w.to(self.device)\n                hairstyle_text_inputs = hairstyle_text_inputs.to(self.device)\n                color_text_inputs = color_text_inputs.to(self.device)\n                hairstyle_tensor = hairstyle_tensor.to(self.device)\n                color_tensor = color_tensor.to(self.device)\n                with torch.no_grad():\n                    x, _ = self.net.decoder([w], input_is_latent=True, randomize_noise=False, truncation=1)\n                if hairstyle_tensor.shape[1] != 1:\n                    hairstyle_tensor_hairmasked = hairstyle_tensor * self.average_color_loss.gen_hair_mask(hairstyle_tensor)\n                else:\n                    hairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                if color_tensor.shape[1] != 1:\n                    color_tensor_hairmasked = color_tensor * self.average_color_loss.gen_hair_mask(color_tensor)\n                else:\n                    color_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                w_hat = w + 0.1 * self.net.mapper(w, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked)\n                x_hat, w_hat = self.net.decoder([w_hat], input_is_latent=True, return_latents=True, randomize_noise=False, truncation=1)\n                loss, loss_dict = self.calc_loss(w, x, w_hat, x_hat, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor, color_tensor, selected_description)\n                loss.backward()\n                self.optimizer.step()\n\n                # Logging related\n                if self.global_step % self.opts.image_interval == 0 or (\n                        self.global_step < 1000 and self.global_step % 1000 == 0):\n                    if (hairstyle_tensor.shape[1] != 1) and (color_tensor.shape[1] != 1):\n                        img_tensor = torch.cat([hairstyle_tensor, color_tensor], dim = 3)\n                    elif hairstyle_tensor.shape[1] != 1:\n                        img_tensor = hairstyle_tensor\n                    elif color_tensor.shape[1] != 1:\n                        img_tensor = color_tensor\n                    else:\n                        img_tensor = None\n                    self.parse_and_log_images(x, x_hat, img_tensor, title='images_train', selected_description=selected_description)\n                if self.global_step % self.opts.board_interval == 0:\n                    self.print_metrics(loss_dict, prefix='train', selected_description=selected_description)\n                    self.log_metrics(loss_dict, prefix='train')\n\n                # Validation related\n                val_loss_dict = None\n                if self.global_step % self.opts.val_interval == 0 or self.global_step == self.opts.max_steps:\n                    val_loss_dict = self.validate()\n                    if val_loss_dict and (self.best_val_loss is None or val_loss_dict['loss'] < self.best_val_loss):\n                        self.best_val_loss = val_loss_dict['loss']\n                        self.checkpoint_me(val_loss_dict, is_best=True)\n\n                if self.global_step % self.opts.save_interval == 0 or self.global_step == self.opts.max_steps:\n                    if val_loss_dict is not None:\n                        self.checkpoint_me(val_loss_dict, is_best=False)\n                    else:\n                        self.checkpoint_me(loss_dict, is_best=False)\n\n                if self.global_step == self.opts.max_steps:\n                    print('OMG, finished training!', flush=True)\n                    break\n\n                self.global_step += 1\n\n    def validate(self):\n        self.net.eval()\n        agg_loss_dict = []\n        for batch_idx, batch in enumerate(self.test_dataloader):\n            if batch_idx > 200:\n                break\n\n            w, hairstyle_text_inputs, color_text_inputs, selected_description_tuple, hairstyle_tensor, color_tensor = batch\n            selected_description = ''\n            for item in selected_description_tuple:\n                selected_description+=item\n\n            with torch.no_grad():\n                w = w.to(self.device).float()\n                hairstyle_text_inputs = hairstyle_text_inputs.to(self.device)\n                color_text_inputs = color_text_inputs.to(self.device)\n                hairstyle_tensor = hairstyle_tensor.to(self.device)\n                color_tensor = color_tensor.to(self.device)\n                x, _ = self.net.decoder([w], input_is_latent=True, randomize_noise=True, truncation=1)\n                if hairstyle_tensor.shape[1] != 1:\n                    hairstyle_tensor_hairmasked = hairstyle_tensor * self.average_color_loss.gen_hair_mask(hairstyle_tensor)\n                else:\n                    hairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                if color_tensor.shape[1] != 1:\n                    color_tensor_hairmasked = color_tensor * self.average_color_loss.gen_hair_mask(color_tensor)\n                else:\n                    color_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n                w_hat = w + 0.1 * self.net.mapper(w, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked)\n                x_hat, _ = self.net.decoder([w_hat], input_is_latent=True, randomize_noise=True, truncation=1)\n                loss, cur_loss_dict = self.calc_loss(w, x, w_hat, x_hat, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor, color_tensor, selected_description)\n            agg_loss_dict.append(cur_loss_dict)\n\n            # Logging related\n            if (hairstyle_tensor.shape[1] != 1) and (color_tensor.shape[1] != 1):\n                img_tensor = torch.cat([hairstyle_tensor, color_tensor], dim = 3)\n            elif hairstyle_tensor.shape[1] != 1:\n                img_tensor = hairstyle_tensor\n            elif color_tensor.shape[1] != 1:\n                img_tensor = color_tensor\n            else:\n                img_tensor = None\n            self.parse_and_log_images(x, x_hat, img_tensor, title='images_val', selected_description=selected_description, index=batch_idx)\n\n            # For first step just do sanity test on small amount of data\n            if self.global_step == 0 and batch_idx >= 4:\n                self.net.train()\n                return None  # Do not log, inaccurate in first batch\n\n        loss_dict = train_utils.aggregate_loss_dict(agg_loss_dict)\n        self.log_metrics(loss_dict, prefix='test')\n        self.print_metrics(loss_dict, prefix='test', selected_description=selected_description)\n\n        self.net.train()\n        return loss_dict\n\n    def checkpoint_me(self, loss_dict, is_best):\n        save_name = 'best_model.pt' if is_best else 'latest_model.pt'\n        save_dict = self.__get_save_dict()\n        checkpoint_path = os.path.join(self.checkpoint_dir, save_name)\n        torch.save(save_dict, checkpoint_path)\n        with open(os.path.join(self.checkpoint_dir, 'timestamp.txt'), 'a') as f:\n            if is_best:\n                f.write('**Best**: Step - {}, Loss - {:.3f} \\n{}\\n'.format(self.global_step, self.best_val_loss, loss_dict))\n            else:\n                f.write('Step - {}, \\n{}\\n'.format(self.global_step, loss_dict))\n\n    def configure_optimizers(self):\n        params = list(self.net.mapper.parameters())\n        if self.opts.optim_name == 'adam':\n            optimizer = torch.optim.Adam(params, lr=self.opts.learning_rate)\n        else:\n            optimizer = Ranger(params, lr=self.opts.learning_rate)\n        return optimizer\n\n    def configure_datasets(self):\n        if self.opts.latents_train_path:\n            train_latents = torch.load(self.opts.latents_train_path)\n        else: \n            train_latents_z = torch.randn(self.opts.train_dataset_size, 512).cuda()\n            train_latents = []\n            for b in range(self.opts.train_dataset_size // self.opts.batch_size):\n                with torch.no_grad():\n                    _, train_latents_b = self.net.decoder([train_latents_z[b: b + self.opts.batch_size]],\n                                                          truncation=0.7, truncation_latent=self.net.latent_avg, return_latents=True)\n                    train_latents.append(train_latents_b)\n            train_latents = torch.cat(train_latents)\n\n        if self.opts.latents_test_path:\n            test_latents = torch.load(self.opts.latents_test_path)\n        else:\n            test_latents_z = torch.randn(self.opts.train_dataset_size, 512).cuda()\n            test_latents = []\n            for b in range(self.opts.test_dataset_size // self.opts.test_batch_size):\n                with torch.no_grad():\n                    _, test_latents_b = self.net.decoder([test_latents_z[b: b + self.opts.test_batch_size]],\n                                                      truncation=0.7, truncation_latent=self.net.latent_avg, return_latents=True)\n                    test_latents.append(test_latents_b)\n            test_latents = torch.cat(test_latents)\n\n        train_dataset_celeba = LatentsDataset(latents=train_latents.cpu(),\n                                              opts=self.opts,\n                                              status='train')\n        test_dataset_celeba = LatentsDataset(latents=test_latents.cpu(),\n                                              opts=self.opts,\n                                              status='test')\n        train_dataset = train_dataset_celeba\n        test_dataset = test_dataset_celeba\n        print(\"Number of training samples: {}\".format(len(train_dataset)), flush=True)\n        print(\"Number of test samples: {}\".format(len(test_dataset)), flush=True)\n        return train_dataset, test_dataset\n\n    def calc_loss(self, w, x, w_hat, x_hat, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor, color_tensor, selected_description):\n        loss_dict = {}\n        loss = 0.0\n        if self.opts.id_lambda > 0:\n            loss_id, sim_improvement = self.id_loss(x_hat, x)\n            loss_dict['loss_id'] = float(loss_id)\n            loss_dict['id_improve'] = float(sim_improvement)\n            loss = loss_id * self.opts.id_lambda * self.opts.attribute_preservation_lambda\n\n        if self.opts.text_manipulation_lambda > 0:\n            if hairstyle_text_inputs.shape[1] != 1:\n                loss_text_hairstyle = self.clip_loss(x_hat, hairstyle_text_inputs).mean()\n                loss_dict['loss_text_hairstyle'] = float(loss_text_hairstyle)\n                loss += loss_text_hairstyle * self.opts.text_manipulation_lambda\n            if color_text_inputs.shape[1] != 1:\n                loss_text_color = self.clip_loss(x_hat, color_text_inputs).mean()\n                loss_dict['loss_text_color'] = float(loss_text_color)\n                loss += loss_text_color * self.opts.text_manipulation_lambda\n\n        if self.opts.image_hairstyle_lambda > 0:\n            if hairstyle_tensor.shape[1] != 1:\n                if 'hairstyle_out_domain_ref' in selected_description:\n                    loss_img_hairstyle = self.image_embedding_loss((x_hat * self.average_color_loss.gen_hair_mask(x_hat)), (hairstyle_tensor * self.average_color_loss.gen_hair_mask(hairstyle_tensor))).mean()\n                    loss_dict['loss_img_hairstyle'] = float(loss_img_hairstyle)\n                    loss += loss_img_hairstyle * self.opts.image_hairstyle_lambda * self.opts.image_manipulation_lambda\n\n        if self.opts.image_color_lambda > 0:\n            if color_tensor.shape[1] != 1:\n                loss_img_color = self.average_color_loss(color_tensor, x_hat)\n                loss_dict['loss_img_color'] = float(loss_img_color)\n                loss += loss_img_color * self.opts.image_color_lambda * self.opts.image_manipulation_lambda\n\n        if self.opts.maintain_color_lambda > 0:\n            if ((hairstyle_tensor.shape[1] != 1) or (hairstyle_text_inputs.shape[1] != 1)) and (color_tensor.shape[1] == 1) and (color_text_inputs.shape[1] == 1):\n                loss_maintain_color_for_hairstyle = self.maintain_color_for_hairstyle_loss(x, x_hat)\n                loss_dict['loss_maintain_color_for_hairstyle'] = float(loss_maintain_color_for_hairstyle)\n                loss += loss_maintain_color_for_hairstyle * self.opts.maintain_color_lambda * self.opts.attribute_preservation_lambda\n        if self.opts.background_lambda > 0:\n            loss_background = self.background_loss(x, x_hat)\n            loss_dict['loss_background'] = float(loss_background)\n            loss += loss_background * self.opts.background_lambda * self.opts.attribute_preservation_lambda\n        if self.opts.latent_l2_lambda > 0:\n            loss_l2_latent = self.latent_l2_loss(w_hat, w)\n            loss_dict['loss_l2_latent'] = float(loss_l2_latent)\n            loss += loss_l2_latent * self.opts.latent_l2_lambda * self.opts.attribute_preservation_lambda\n        loss_dict['loss'] = float(loss)\n        return loss, loss_dict\n\n    def log_metrics(self, metrics_dict, prefix):\n        for key, value in metrics_dict.items():\n            self.logger.add_scalar('{}/{}'.format(prefix, key), value, self.global_step)\n\n    def print_metrics(self, metrics_dict, prefix, selected_description):\n        if prefix == 'train':\n            print('Metrics for {}, step {}'.format(prefix, self.global_step), selected_description, flush=True)\n        else:\n            print('Metrics for {}, step {}'.format(prefix, self.global_step), flush=True)\n        for key, value in metrics_dict.items():\n            print('\\t{} = '.format(key), value, flush=True)\n\n    def parse_and_log_images(self, x, x_hat, img_tensor, title, selected_description, index=None):\n        if index is None:\n            path = os.path.join(self.log_dir, title, f'{str(self.global_step).zfill(5)}-{selected_description}.jpg')\n        else:\n            path = os.path.join(self.log_dir, title, f'{str(self.global_step).zfill(5)}-{str(index).zfill(5)}-{selected_description}.jpg')\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        if img_tensor is not None:\n            if img_tensor.shape[3] == 1024:\n                torchvision.utils.save_image(torch.cat([x.detach().cpu(), x_hat.detach().cpu(), img_tensor.detach().cpu()]), path,\n                                     normalize=True, scale_each=True, value_range=(-1, 1), nrow=3)\n            elif img_tensor.shape[3] == 2048:\n                torchvision.utils.save_image(torch.cat([x.detach().cpu(), x_hat.detach().cpu(), img_tensor[:,:,:,0:1024].detach().cpu(), img_tensor[:,:,:,1024::].detach().cpu()]), path,\n                                     normalize=True, scale_each=True, value_range=(-1, 1), nrow=4)\t\t\t\t\n        else:\n            torchvision.utils.save_image(torch.cat([x.detach().cpu(), x_hat.detach().cpu()]), path,\n                                     normalize=True, scale_each=True, value_range=(-1, 1), nrow=2)\t\t\t\t\n\n    def __get_save_dict(self):\n        save_dict = {\n            'state_dict': self.net.state_dict(),\n            'opts': vars(self.opts)\n        }\n        return save_dict","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:16.945215Z","iopub.execute_input":"2024-05-02T11:04:16.945664Z","iopub.status.idle":"2024-05-02T11:04:17.014795Z","shell.execute_reply.started":"2024-05-02T11:04:16.945637Z","shell.execute_reply":"2024-05-02T11:04:17.014101Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Train.py","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport sys\nimport pprint\n\n# sys.path.insert(0, \"/kaggle/working/HairCLIP/mapper\")\n# from mapper.options.train_options import TrainOptions","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:17.127566Z","iopub.execute_input":"2024-05-02T11:04:17.128044Z","iopub.status.idle":"2024-05-02T11:04:17.132321Z","shell.execute_reply.started":"2024-05-02T11:04:17.128019Z","shell.execute_reply":"2024-05-02T11:04:17.131444Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from argparse import ArgumentParser\n\n\nclass TrainOptions:\n\n    def __init__(self):\n        self.parser = ArgumentParser()\n        self.initialize()\n\n    def initialize(self):\n        self.parser.add_argument('--exp_dir', default=\"/kaggle/working/experiment\", type=str, help='Path to experiment output directory')\n        self.parser.add_argument('--no_coarse_mapper', default=False, action=\"store_true\")\n        self.parser.add_argument('--no_medium_mapper', default=False, action=\"store_true\")\n        self.parser.add_argument('--no_fine_mapper', default=False, action=\"store_true\")\n        self.parser.add_argument('--latents_train_path', default=\"/kaggle/working/HairCLIP/mapper/datasets/train_faces.pt\", type=str, help=\"The latents for the training\")\n        self.parser.add_argument('--latents_test_path', default=\"/kaggle/working/HairCLIP/mapper/datasets/test_faces.pt\", type=str, help=\"The latents for the validation\")\n\n        self.parser.add_argument('--hairstyle_ref_img_train_path', default=\"/kaggle/input/celebahq-resized-256x256/celeba_hq_256\", type=str, help=\"The hairstyle reference image for the training\")\n        self.parser.add_argument('--hairstyle_ref_img_test_path', default=\"/kaggle/input/celebahq-resized-256x256/celeba_hq_256\", type=str, help=\"The hairstyle reference image for the validation\")\n        self.parser.add_argument('--color_ref_img_train_path', default=\"/kaggle/input/celebahq-resized-256x256/celeba_hq_256\", type=str, help=\"The color reference image for the training\")\n        self.parser.add_argument('--color_ref_img_test_path', default=\"/kaggle/input/celebahq-resized-256x256/celeba_hq_256\", type=str, help=\"The color reference image for the validation\")\n        self.parser.add_argument('--color_ref_img_in_domain_path', default=\"\", type=str, help=\"The color reference image in domain for the augmentation\")\n        self.parser.add_argument('--num_for_each_augmented_color', default=4000, type=int, help='Number for each augmented color')\n\n        self.parser.add_argument('--hairstyle_manipulation_prob', default=0.5, type=float, help='Probability of only manipulating the hairstyle')\n        self.parser.add_argument('--color_manipulation_prob', default=0.2, type=float, help='Probability of only manipulating the color')\n        self.parser.add_argument('--both_manipulation_prob', default=0.27, type=float, help='Probability of simultaneously manipulating hairstyle and color')\n\n        self.parser.add_argument('--hairstyle_text_manipulation_prob', default=0.5, type=float, help='Probability of using text to manipulate hairstyle')\n        self.parser.add_argument('--color_text_manipulation_prob', default=0.5, type=float, help='Probability of using text to manipulate color')\n        self.parser.add_argument('--color_in_domain_ref_manipulation_prob', default=0, type=float, help='Probability of using in-domain reference image to manipulate color')\n\n        self.parser.add_argument('--train_dataset_size', default=5000, type=int, help=\"Will be used only if no latents are given\")\n        self.parser.add_argument('--test_dataset_size', default=1000, type=int, help=\"Will be used only if no latents are given\")\n\n        self.parser.add_argument('--batch_size', default=1, type=int, help='Batch size for training')\n        self.parser.add_argument('--test_batch_size', default=1, type=int, help='Batch size for testing and inference')\n        self.parser.add_argument('--workers', default=4, type=int, help='Number of train dataloader workers')\n        self.parser.add_argument('--test_workers', default=2, type=int, help='Number of test/inference dataloader workers')\n\n        self.parser.add_argument('--learning_rate', default=0.0005, type=float, help='Optimizer learning rate')\n        self.parser.add_argument('--optim_name', default='ranger', type=str, help='Which optimizer to use')\n\n\n        self.parser.add_argument('--text_manipulation_lambda', default=2.0, type=float, help='Text manipulation loss multiplier factor')\n        self.parser.add_argument('--image_manipulation_lambda', default=1.0, type=float, help='Image manipulation loss multiplier factor')\n        self.parser.add_argument('--attribute_preservation_lambda', default=1.0, type=float, help='Attribute preservation loss multiplier factor')\n\n\n        self.parser.add_argument('--image_hairstyle_lambda', default=5.0, type=float, help='Image-based hairstyle manipulation loss multiplier factor')\n        self.parser.add_argument('--image_color_lambda', default=0.02, type=float, help='Image-based color manipulation loss multiplier factor')\n\n        self.parser.add_argument('--id_lambda', default=0.3, type=float, help='ID loss multiplier factor')\n        self.parser.add_argument('--maintain_color_lambda', default=0.02, type=float, help='Color retention loss multiplier factor')\n        self.parser.add_argument('--background_lambda', default=1.0, type=float, help='Background loss multiplier factor')\n        self.parser.add_argument('--latent_l2_lambda', default=0.8, type=float, help='Latent L2 loss multiplier factor')\n\n        self.parser.add_argument('--parsenet_weights', default='/kaggle/working/HairCLIP/pretrained_models/parsenet.pth', type=str, help='Path to Parsing model weights')\n        self.parser.add_argument('--stylegan_weights', default='/kaggle/working/HairCLIP/pretrained_models/stylegan2-ffhq-config-f.pt', type=str, help='Path to StyleGAN model weights')\n        self.parser.add_argument('--stylegan_size', default=1024, type=int)\n        self.parser.add_argument('--ir_se50_weights', default='/kaggle/working/HairCLIP/pretrained_models/model_ir_se50.pth', type=str, help=\"Path to facial recognition network used in ID loss\")\n        self.parser.add_argument('--checkpoint_path', default=\"/kaggle/working/HairCLIP/pretrained_models/hairclip.pt\", type=str, help='Path to HairCLIP model checkpoint')\n\n        self.parser.add_argument('--max_steps', default=1000, type=int, help='Maximum number of training steps')\n        self.parser.add_argument('--image_interval', default=100, type=int, help='Interval for logging train images during training')\n        self.parser.add_argument('--board_interval', default=50, type=int, help='Interval for logging metrics to tensorboard')\n        self.parser.add_argument('--val_interval', default=2000, type=int, help='Validation interval')\n        self.parser.add_argument('--save_interval', default=2000, type=int, help='Model checkpoint interval')\n\n        self.parser.add_argument('--hairstyle_description', default=\"/kaggle/working/HairCLIP/mapper/hairstyle_list.txt\", type=str, help='Hairstyle text prompt list')\n        self.parser.add_argument('--color_description', default = \"purple, red, orange, yellow, green, blue, gray, brown, black, white, blond, pink\", type=str, help='Color text prompt, eg: purple, red, orange')\n\n\n    def parse(self):\n        opts = self.parser.parse_args([])\n        return opts","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:17.330333Z","iopub.execute_input":"2024-05-02T11:04:17.330666Z","iopub.status.idle":"2024-05-02T11:04:17.355033Z","shell.execute_reply.started":"2024-05-02T11:04:17.330641Z","shell.execute_reply":"2024-05-02T11:04:17.354130Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"opts = TrainOptions().parse()\nif opts.batch_size != 1 or opts.test_batch_size != 1:\n    raise Exception('This version only supports batch size and test batch size to be 1.')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:18.002854Z","iopub.execute_input":"2024-05-02T11:04:18.003720Z","iopub.status.idle":"2024-05-02T11:04:18.012691Z","shell.execute_reply.started":"2024-05-02T11:04:18.003680Z","shell.execute_reply":"2024-05-02T11:04:18.010814Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import shutil\ndef remove_folder(folder_path):\n    shutil.rmtree(folder_path)\n\nif os.path.exists(\"/kaggle/working/experiment\"):\n    remove_folder(\"/kaggle/working/experiment\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:18.394580Z","iopub.execute_input":"2024-05-02T11:04:18.395322Z","iopub.status.idle":"2024-05-02T11:04:18.400124Z","shell.execute_reply.started":"2024-05-02T11:04:18.395292Z","shell.execute_reply":"2024-05-02T11:04:18.399120Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(opts.exp_dir):\n    raise Exception('Oops... {} already exists'.format(opts.exp_dir))\n    \nos.makedirs(opts.exp_dir, exist_ok=True)\n\nopts_dict = vars(opts)\npprint.pprint(opts_dict)\nwith open(os.path.join(opts.exp_dir, 'opt.json'), 'w') as f:\n    json.dump(opts_dict, f, indent=4, sort_keys=True)\n\ncoach = Coach(opts)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:04:19.178117Z","iopub.execute_input":"2024-05-02T11:04:19.178568Z","iopub.status.idle":"2024-05-02T11:05:06.051633Z","shell.execute_reply.started":"2024-05-02T11:04:19.178524Z","shell.execute_reply":"2024-05-02T11:05:06.050630Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"{'attribute_preservation_lambda': 1.0,\n 'background_lambda': 1.0,\n 'batch_size': 1,\n 'board_interval': 50,\n 'both_manipulation_prob': 0.27,\n 'checkpoint_path': '/kaggle/working/HairCLIP/pretrained_models/hairclip.pt',\n 'color_description': 'purple, red, orange, yellow, green, blue, gray, brown, '\n                      'black, white, blond, pink',\n 'color_in_domain_ref_manipulation_prob': 0,\n 'color_manipulation_prob': 0.2,\n 'color_ref_img_in_domain_path': '',\n 'color_ref_img_test_path': '/kaggle/input/celebahq-resized-256x256/celeba_hq_256',\n 'color_ref_img_train_path': '/kaggle/input/celebahq-resized-256x256/celeba_hq_256',\n 'color_text_manipulation_prob': 0.5,\n 'exp_dir': '/kaggle/working/experiment',\n 'hairstyle_description': '/kaggle/working/HairCLIP/mapper/hairstyle_list.txt',\n 'hairstyle_manipulation_prob': 0.5,\n 'hairstyle_ref_img_test_path': '/kaggle/input/celebahq-resized-256x256/celeba_hq_256',\n 'hairstyle_ref_img_train_path': '/kaggle/input/celebahq-resized-256x256/celeba_hq_256',\n 'hairstyle_text_manipulation_prob': 0.5,\n 'id_lambda': 0.3,\n 'image_color_lambda': 0.02,\n 'image_hairstyle_lambda': 5.0,\n 'image_interval': 100,\n 'image_manipulation_lambda': 1.0,\n 'ir_se50_weights': '/kaggle/working/HairCLIP/pretrained_models/model_ir_se50.pth',\n 'latent_l2_lambda': 0.8,\n 'latents_test_path': '/kaggle/working/HairCLIP/mapper/datasets/test_faces.pt',\n 'latents_train_path': '/kaggle/working/HairCLIP/mapper/datasets/train_faces.pt',\n 'learning_rate': 0.0005,\n 'maintain_color_lambda': 0.02,\n 'max_steps': 500000,\n 'no_coarse_mapper': False,\n 'no_fine_mapper': False,\n 'no_medium_mapper': False,\n 'num_for_each_augmented_color': 4000,\n 'optim_name': 'ranger',\n 'parsenet_weights': '/kaggle/working/HairCLIP/pretrained_models/parsenet.pth',\n 'save_interval': 2000,\n 'stylegan_size': 1024,\n 'stylegan_weights': '/kaggle/working/HairCLIP/pretrained_models/stylegan2-ffhq-config-f.pt',\n 'test_batch_size': 1,\n 'test_dataset_size': 1000,\n 'test_workers': 2,\n 'text_manipulation_lambda': 2.0,\n 'train_dataset_size': 5000,\n 'val_interval': 2000,\n 'workers': 4}\nLoading from checkpoint: /kaggle/working/HairCLIP/pretrained_models/hairclip.pt\nLoading ResNet ArcFace for ID Loss\nLoading UNet for Background Loss\nLoading UNet for AvgLabLoss\nLoading UNet for AvgLabLoss\nNumber of training samples: 24176\nNumber of test samples: 2824\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check Dataloader shape","metadata":{}},{"cell_type":"code","source":"w, hairstyle_text_inputs, color_text_inputs, selected_description_tuple, hairstyle_tensor, color_tensor = next(iter(coach.train_dataloader))\nprint(w.shape)\nprint(hairstyle_text_inputs.shape)\nprint(color_text_inputs.shape)\nprint(selected_description_tuple) \nprint(hairstyle_tensor.shape)\nprint(color_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:05:06.053865Z","iopub.execute_input":"2024-05-02T11:05:06.054200Z","iopub.status.idle":"2024-05-02T11:05:06.830785Z","shell.execute_reply.started":"2024-05-02T11:05:06.054171Z","shell.execute_reply":"2024-05-02T11:05:06.829625Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"torch.Size([1, 18, 512])\ntorch.Size([1, 1])\ntorch.Size([1, 1])\n('hairstyle_out_domain_ref-color_out_domain_ref',)\ntorch.Size([1, 3, 1024, 1024])\ntorch.Size([1, 3, 1024, 1024])\n","output_type":"stream"}]},{"cell_type":"code","source":"w, hairstyle_text_inputs, color_text_inputs, selected_description_tuple, hairstyle_tensor, color_tensor = next(iter(coach.test_dataloader))\nprint(w.shape)\nprint(hairstyle_text_inputs.shape)\nprint(color_text_inputs.shape)\nprint(selected_description_tuple) \nprint(hairstyle_tensor.shape)\nprint(color_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:05:06.832190Z","iopub.execute_input":"2024-05-02T11:05:06.832497Z","iopub.status.idle":"2024-05-02T11:05:07.132052Z","shell.execute_reply.started":"2024-05-02T11:05:06.832471Z","shell.execute_reply":"2024-05-02T11:05:07.130806Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"torch.Size([1, 18, 512])\ntorch.Size([1, 77])\ntorch.Size([1, 1])\n('pageboy hairstyle',)\ntorch.Size([1, 1])\ntorch.Size([1, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check propagation","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    for batch_idx, batch in enumerate(coach.test_dataloader):\n        w, hairstyle_text_inputs, color_text_inputs, selected_description_tuple, hairstyle_tensor, color_tensor = batch\n        selected_description = ''\n        for item in selected_description_tuple:\n            selected_description+=item\n\n        w = w.to(coach.device)\n        hairstyle_text_inputs = hairstyle_text_inputs.to(coach.device)\n        color_text_inputs = color_text_inputs.to(coach.device)\n        hairstyle_tensor = hairstyle_tensor.to(coach.device)\n        color_tensor = color_tensor.to(coach.device)\n        \n        x, _ = coach.net.decoder([w], input_is_latent=True, randomize_noise=False, truncation=1)\n        if hairstyle_tensor.shape[1] != 1:\n            hairstyle_tensor_hairmasked = hairstyle_tensor * coach.average_color_loss.gen_hair_mask(hairstyle_tensor)\n        else:\n            hairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n        if color_tensor.shape[1] != 1:\n            color_tensor_hairmasked = color_tensor * coach.average_color_loss.gen_hair_mask(color_tensor)\n        else:\n            color_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n        w_hat = w + 0.1 * coach.net.mapper(w, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked)\n        x_hat, w_hat = coach.net.decoder([w_hat], input_is_latent=True, return_latents=True, randomize_noise=False, truncation=1)\n        \n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:08:21.708377Z","iopub.execute_input":"2024-05-02T11:08:21.709051Z","iopub.status.idle":"2024-05-02T11:08:22.140598Z","shell.execute_reply.started":"2024-05-02T11:08:21.709015Z","shell.execute_reply":"2024-05-02T11:08:22.139255Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"print(w.shape)\nprint(w_hat.shape)\nprint(x.shape)\nprint(x_hat.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:08:16.975406Z","iopub.execute_input":"2024-05-02T11:08:16.976223Z","iopub.status.idle":"2024-05-02T11:08:16.981824Z","shell.execute_reply.started":"2024-05-02T11:08:16.976186Z","shell.execute_reply":"2024-05-02T11:08:16.980920Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"torch.Size([1, 18, 512])\ntorch.Size([1, 18, 512])\ntorch.Size([1, 3, 1024, 1024])\ntorch.Size([1, 3, 1024, 1024])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"coach.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}