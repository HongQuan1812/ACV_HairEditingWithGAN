{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2177371,"sourceType":"datasetVersion","datasetId":1307206},{"sourceId":5815323,"sourceType":"datasetVersion","datasetId":3341018}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SetUp FS_encoder","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define a function to remove a directory and its contents recursively\ndef remove_folder(folder_path):\n    shutil.rmtree(folder_path)\n\n# Example usage:\nif os.path.exists('/kaggle/working/FeatureStyleEncoder'):\n    remove_folder('/kaggle/working/FeatureStyleEncoder')\n    \n%cd /kaggle/working/\n!git clone https://github.com/InterDigitalInc/FeatureStyleEncoder.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install face_alignment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd FeatureStyleEncoder\n\n!pip install gdown\n%mkdir pretrained_models\n%cd pretrained_models\n\n# download pretrained encoder\n!gdown --fuzzy https://drive.google.com/file/d/1RnnBL77j_Can0dY1KOiXHvG224MxjvzC/view?usp=sharing\n\n# download arcface pretrained model\n!gdown --fuzzy https://drive.google.com/file/d/1coFTz-Kkgvoc_gRT8JFzqCgeC3lAFWQp/view?usp=sharing\n\n# download face parsing model from https://github.com/zllrunning/face-parsing.PyTorch\n!gdown --fuzzy https://drive.google.com/open?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812\n    \n# download pSp pretrained model from https://github.com/eladrich/pixel2style2pixel.git\n%cd ../pixel2style2pixel\n!mkdir pretrained_models\n\n%cd pretrained_models\n!gdown --fuzzy https://drive.google.com/file/d/1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0/view?usp=sharing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyDrive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/FeatureStyleEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/FeatureStyleEncoder\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport argparse\nimport glob\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport yaml\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms, utils\nfrom utils.functions import *\n\nfrom trainer import *","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:37.876982Z","iopub.execute_input":"2024-05-01T07:27:37.877927Z","iopub.status.idle":"2024-05-01T07:27:40.728013Z","shell.execute_reply.started":"2024-05-01T07:27:37.877890Z","shell.execute_reply":"2024-05-01T07:27:40.727006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.enabled = True\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(True)\nImage.MAX_IMAGE_PIXELS = None\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--config', type=str, default='001', help='Path to the config file.')\nparser.add_argument('--pretrained_model_path', type=str, default='./pretrained_models/143_enc.pth', help='pretrained stylegan2 model')\nparser.add_argument('--stylegan_model_path', type=str, default='./pixel2style2pixel/pretrained_models/psp_ffhq_encode.pt', help='pretrained stylegan2 model')\nparser.add_argument('--arcface_model_path', type=str, default='./pretrained_models/backbone.pth', help='pretrained arcface model')\nparser.add_argument('--parsing_model_path', type=str, default='./pretrained_models/79999_iter.pth', help='pretrained parsing model')\nparser.add_argument('--log_path', type=str, default='./logs/', help='log file path')\nparser.add_argument('--resume', action='store_true', help='resume from checkpoint')\nparser.add_argument('--checkpoint', type=str, default='', help='checkpoint file path')\nparser.add_argument('--checkpoint_noiser', type=str, default='', help='checkpoint file path')\nparser.add_argument('--multigpu', type=bool, default=True, help='use multiple gpus')\nparser.add_argument('--input_path', type=str, default='./test/', help='evaluation data file path')\nparser.add_argument('--save_path', type=str, default='./', help='output data save path')\nfs_opts = parser.parse_args([])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:40.730129Z","iopub.execute_input":"2024-05-01T07:27:40.730654Z","iopub.status.idle":"2024-05-01T07:27:40.743177Z","shell.execute_reply.started":"2024-05-01T07:27:40.730626Z","shell.execute_reply":"2024-05-01T07:27:40.742010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = yaml.load(open('./configs/' + fs_opts.config + '.yaml', 'r'), Loader=yaml.FullLoader)\n\n# Initialize trainer\ntrainer = Trainer(config, fs_opts)\ntrainer.initialize(fs_opts.stylegan_model_path, fs_opts.arcface_model_path, fs_opts.parsing_model_path)  \ntrainer.to(device)\n\nstate_dict = torch.load(fs_opts.pretrained_model_path)#os.path.join(fs_opts.log_path, fs_opts.config + '/checkpoint.pth'))\ntrainer.enc.load_state_dict(torch.load(fs_opts.pretrained_model_path))\ntrainer.enc.eval()\n\nprint(\"Feature_style_encoder successfully loaded!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:40.744388Z","iopub.execute_input":"2024-05-01T07:27:40.744738Z","iopub.status.idle":"2024-05-01T07:27:52.322026Z","shell.execute_reply.started":"2024-05-01T07:27:40.744704Z","shell.execute_reply":"2024-05-01T07:27:52.321025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make dataset: 10000 bức ảnh đầu trong ffhq","metadata":{}},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:52.324647Z","iopub.execute_input":"2024-05-01T07:27:52.324973Z","iopub.status.idle":"2024-05-01T07:27:52.329676Z","shell.execute_reply.started":"2024-05-01T07:27:52.324929Z","shell.execute_reply":"2024-05-01T07:27:52.328720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a target_dir and transform (optional) parameter\n    def __init__(self, target_dir: str, \n                 range_of_samples: tuple, \n                 transform=None) -> None:\n        \n    # 3. Create class attributes\n        self.image_dir_path = target_dir\n        # Get all image names\n        head, tail = range_of_samples\n        self.image_names = os.listdir(target_dir)[head:tail]\n        # Setup transforms\n        self.transform = transform\n        \n    # 4. Make function to load images\n    def load_image(self, index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = os.path.join(self.image_dir_path, self.image_names[index])\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.image_names)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n        img = self.load_image(index)\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img)\n        else:\n            return img","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:52.330855Z","iopub.execute_input":"2024-05-01T07:27:52.331184Z","iopub.status.idle":"2024-05-01T07:27:52.340607Z","shell.execute_reply.started":"2024-05-01T07:27:52.331159Z","shell.execute_reply":"2024-05-01T07:27:52.339700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_tensor = transforms.Compose([\n    transforms.Resize((1024, 1024)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:52.341577Z","iopub.execute_input":"2024-05-01T07:27:52.341838Z","iopub.status.idle":"2024-05-01T07:27:52.352484Z","shell.execute_reply.started":"2024-05-01T07:27:52.341815Z","shell.execute_reply":"2024-05-01T07:27:52.351436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir_path = \"/kaggle/input/flickr-faces-hq-dataset-ffhq/images1024x1024\"\n\ndata_custom = ImageFolderCustom(target_dir=image_dir_path,\n                                range_of_samples = (0, 10_000),\n                                transform=img_to_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:52.353747Z","iopub.execute_input":"2024-05-01T07:27:52.354032Z","iopub.status.idle":"2024-05-01T07:27:53.084226Z","shell.execute_reply.started":"2024-05-01T07:27:52.354009Z","shell.execute_reply":"2024-05-01T07:27:53.083064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_custom)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:53.086030Z","iopub.execute_input":"2024-05-01T07:27:53.086359Z","iopub.status.idle":"2024-05-01T07:27:53.093686Z","shell.execute_reply.started":"2024-05-01T07:27:53.086333Z","shell.execute_reply":"2024-05-01T07:27:53.092477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_custom[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:53.094936Z","iopub.execute_input":"2024-05-01T07:27:53.095273Z","iopub.status.idle":"2024-05-01T07:27:53.183263Z","shell.execute_reply.started":"2024-05-01T07:27:53.095247Z","shell.execute_reply":"2024-05-01T07:27:53.182205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare Data loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport os\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 4\nNUM_CORES = os.cpu_count()\n\n# Turn datasets into iterables (batches)\ndataloader = DataLoader(\n    data_custom, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=False, # shuffle data every epoch?\n    \n    num_workers = NUM_CORES,\n    pin_memory = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:53.185967Z","iopub.execute_input":"2024-05-01T07:27:53.186280Z","iopub.status.idle":"2024-05-01T07:27:53.191622Z","shell.execute_reply.started":"2024-05-01T07:27:53.186255Z","shell.execute_reply":"2024-05-01T07:27:53.190763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:53.192973Z","iopub.execute_input":"2024-05-01T07:27:53.193373Z","iopub.status.idle":"2024-05-01T07:27:53.205275Z","shell.execute_reply.started":"2024-05-01T07:27:53.193340Z","shell.execute_reply":"2024-05-01T07:27:53.204264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch = next(iter(dataloader))\nimage_batch.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:27:53.206528Z","iopub.execute_input":"2024-05-01T07:27:53.206847Z","iopub.status.idle":"2024-05-01T07:27:54.623109Z","shell.execute_reply.started":"2024-05-01T07:27:53.206821Z","shell.execute_reply":"2024-05-01T07:27:54.621956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%mkdir /kaggle/working/features\n%mkdir /kaggle/working/latents","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:31:31.535460Z","iopub.execute_input":"2024-05-01T07:31:31.535843Z","iopub.status.idle":"2024-05-01T07:31:33.657598Z","shell.execute_reply.started":"2024-05-01T07:31:31.535812Z","shell.execute_reply":"2024-05-01T07:31:33.656288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\n\nwith torch.no_grad():\n    latents = []; features = []\n    for batch, image in tqdm(enumerate(dataloader), total = len(dataloader)):\n#        print(batch)\n#        print(image.shape)\n        output = trainer.test(img=image.to(device), return_latent=True)\n        feature = output.pop()\n        latent = output.pop()\n#         print(feature.shape)\n#         print(latent.shape)\n        latents.append(latent)\n        features.append(feature)\n        \n    latents = torch.cat(latents)\n    features = torch.cat(features)\n    torch.save(latents, f'/kaggle/working/latents/latents{10_000}.pt')\n    torch.save(features, f'/kaggle/working/features/features{10_000}.pt')\n    latents = []; features = []","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:31:53.812671Z","iopub.execute_input":"2024-05-01T07:31:53.813095Z","iopub.status.idle":"2024-05-01T07:50:31.039785Z","shell.execute_reply.started":"2024-05-01T07:31:53.813055Z","shell.execute_reply":"2024-05-01T07:50:31.038314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make dataset: 20000 ","metadata":{}},{"cell_type":"code","source":"data_custom = ImageFolderCustom(target_dir=image_dir_path,\n                                range_of_samples = (10_000, 20_000),\n                                transform=img_to_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:31.042680Z","iopub.execute_input":"2024-05-01T07:50:31.043626Z","iopub.status.idle":"2024-05-01T07:50:31.081615Z","shell.execute_reply.started":"2024-05-01T07:50:31.043578Z","shell.execute_reply":"2024-05-01T07:50:31.080584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_custom)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:31.083346Z","iopub.execute_input":"2024-05-01T07:50:31.083732Z","iopub.status.idle":"2024-05-01T07:50:41.543382Z","shell.execute_reply.started":"2024-05-01T07:50:31.083697Z","shell.execute_reply":"2024-05-01T07:50:41.542029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_custom[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:41.546945Z","iopub.execute_input":"2024-05-01T07:50:41.547492Z","iopub.status.idle":"2024-05-01T07:50:41.640343Z","shell.execute_reply.started":"2024-05-01T07:50:41.547446Z","shell.execute_reply":"2024-05-01T07:50:41.639232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare DataLoader","metadata":{}},{"cell_type":"code","source":"# Setup the batch size hyperparameter\nBATCH_SIZE = 4\nNUM_CORES = os.cpu_count()\n\n# Turn datasets into iterables (batches)\ndataloader = DataLoader(\n    data_custom, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=False, # shuffle data every epoch?\n    \n    num_workers = NUM_CORES,\n    pin_memory = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:41.641693Z","iopub.execute_input":"2024-05-01T07:50:41.642451Z","iopub.status.idle":"2024-05-01T07:50:41.649219Z","shell.execute_reply.started":"2024-05-01T07:50:41.642420Z","shell.execute_reply":"2024-05-01T07:50:41.648267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:41.650330Z","iopub.execute_input":"2024-05-01T07:50:41.651662Z","iopub.status.idle":"2024-05-01T07:50:41.789980Z","shell.execute_reply.started":"2024-05-01T07:50:41.651466Z","shell.execute_reply":"2024-05-01T07:50:41.788861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch = next(iter(dataloader))\nimage_batch.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:50:41.792210Z","iopub.execute_input":"2024-05-01T07:50:41.792767Z","iopub.status.idle":"2024-05-01T07:50:43.342864Z","shell.execute_reply.started":"2024-05-01T07:50:41.792728Z","shell.execute_reply":"2024-05-01T07:50:43.341644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\ntorch.cuda.empty_cache()\nwith torch.no_grad():\n    latents = []; features = []\n    for batch, image in tqdm(enumerate(dataloader), total = len(dataloader)):\n#        print(batch)\n#        print(image.shape)\n        output = trainer.test(img=image.to(device), return_latent=True)\n        feature = output.pop()\n        latent = output.pop()\n#         print(feature.shape)\n#         print(latent.shape)\n        latents.append(latent)\n        features.append(feature)\n        \n    latents = torch.cat(latents)\n    features = torch.cat(features)\n    torch.save(latents, f'/kaggle/working/latents/latents{20_000}.pt')\n    torch.save(features, f'/kaggle/working/features/features{20_000}.pt')\n    latents = []; features = []","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:54:32.799469Z","iopub.execute_input":"2024-05-01T07:54:32.800242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make dataset: 30000 ","metadata":{}},{"cell_type":"code","source":"data_custom = ImageFolderCustom(target_dir=image_dir_path,\n                                range_of_samples = (20_000, 30_000),\n                                transform=img_to_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_custom)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_custom[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare DataLoader","metadata":{}},{"cell_type":"code","source":"# Setup the batch size hyperparameter\nBATCH_SIZE = 4\nNUM_CORES = os.cpu_count()\n\n# Turn datasets into iterables (batches)\ndataloader = DataLoader(\n    data_custom, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=False, # shuffle data every epoch?\n    \n    num_workers = NUM_CORES,\n    pin_memory = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch = next(iter(dataloader))\nimage_batch.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\ntorch.cuda.empty_cache()\nwith torch.no_grad():\n    latents = []; features = []\n    for batch, image in tqdm(enumerate(dataloader), total = len(dataloader)):\n#        print(batch)\n#        print(image.shape)\n        output = trainer.test(img=image.to(device), return_latent=True)\n        feature = output.pop()\n        latent = output.pop()\n#         print(feature.shape)\n#         print(latent.shape)\n        latents.append(latent)\n        features.append(feature)\n        \n    latents = torch.cat(latents)\n    features = torch.cat(features)\n    torch.save(latents, f'/kaggle/working/latents/latents{30_000}.pt')\n    torch.save(features, f'/kaggle/working/features/features{30_000}.pt')\n    latents = []; features = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_folder(\"/kaggle/working/FeatureStyleEncoder\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}